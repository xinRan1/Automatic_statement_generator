{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object: Train an LSTM to mimic Russellâ€™s style and thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download 4 books written by Bertrand Russell\n",
    "\n",
    "2. Import these text files in and reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('../data/1.txt','r', encoding=\"ISO-8859-1\").read()\n",
    "f1 = f1.lower()\n",
    "f1 = f1.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open('../data/2.txt','r', encoding=\"ISO-8859-1\").read()\n",
    "f2 = f2.lower()\n",
    "f2 = f2.replace('\\n',' ')\n",
    "f3 = open('../data/3.txt','r', encoding=\"ISO-8859-1\").read()\n",
    "f3 = f3.lower()\n",
    "f3 = f3.replace('\\n',' ')\n",
    "f4 = open('../data/4.txt','r', encoding=\"ISO-8859-1\").read()\n",
    "f4 = f4.lower()\n",
    "f4 = f4.replace('\\n',' ')\n",
    "\n",
    "# CONCANTENATE 4 FILES TOGEETHER \n",
    "files = f1+f2+f3+f4\n",
    "files = files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations in the concatenated text\n",
    "files = files.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Encode characters in ASCII order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_chars = sorted(list(set(files)))\n",
    "char_ind = dict()\n",
    "for ind, char in enumerate(sorted_chars):\n",
    "    char_ind[char] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 73 characters used in the text.\n"
     ]
    }
   ],
   "source": [
    "# report the number of all characters used\n",
    "chars_used = len(char_ind)\n",
    "print(f'There are totally {chars_used} characters used in the text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 1548600 characters in total.\n"
     ]
    }
   ],
   "source": [
    "total_chars = len(files)\n",
    "print(f'The file contains {total_chars} characters in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Rescale characters and set up input, output sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 100-1\n",
    "input_sets = []\n",
    "output_sets = []\n",
    "for i in range(0, total_chars - win):\n",
    "    try:\n",
    "        eles_in = files[i:i + win]\n",
    "        ele_out = files[i + win]\n",
    "    except:\n",
    "        continue\n",
    "    input_lst = []\n",
    "    for c in eles_in:\n",
    "        input_lst.append(char_ind[c])\n",
    "    input_sets.append(input_lst)\n",
    "    output_sets.append(char_ind[ele_out])\n",
    "# total number of input sets\n",
    "num_input_sets = len(input_sets)\n",
    "data_X = np.reshape(input_sets, (num_input_sets, win, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaled input numbers\n",
    "data_X = data_X / float(chars_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the output with the one-hot encoding scheme\n",
    "data_y = np_utils.to_categorical(output_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build a single hidden layer for the LSTM with N = 256 (or less) memory units; use Softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(256, input_shape=(data_X.shape[1], data_X.shape[2])))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(Dense(data_y.shape[1], activation='softmax'))\n",
    "lstm.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 73)                18761     \n",
      "=================================================================\n",
      "Total params: 282,953\n",
      "Trainable params: 282,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Choose 16 epochs for training; use model checkpointing to keep the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weights_file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_lst = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "12098/12098 [==============================] - 2780s 230ms/step - loss: 2.5590\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.55896, saving model to weights-improvement-01-2.5590.hdf5\n",
      "Epoch 2/16\n",
      "12098/12098 [==============================] - 4739s 392ms/step - loss: 2.2389\n",
      "\n",
      "Epoch 00002: loss improved from 2.55896 to 2.23888, saving model to weights-improvement-02-2.2389.hdf5\n",
      "Epoch 3/16\n",
      "12098/12098 [==============================] - 2607s 215ms/step - loss: 2.0508\n",
      "\n",
      "Epoch 00003: loss improved from 2.23888 to 2.05078, saving model to weights-improvement-03-2.0508.hdf5\n",
      "Epoch 4/16\n",
      "12098/12098 [==============================] - 2595s 215ms/step - loss: 1.9453\n",
      "\n",
      "Epoch 00004: loss improved from 2.05078 to 1.94533, saving model to weights-improvement-04-1.9453.hdf5\n",
      "Epoch 5/16\n",
      "12098/12098 [==============================] - 2594s 214ms/step - loss: 1.8770\n",
      "\n",
      "Epoch 00005: loss improved from 1.94533 to 1.87700, saving model to weights-improvement-05-1.8770.hdf5\n",
      "Epoch 6/16\n",
      "12098/12098 [==============================] - 2616s 216ms/step - loss: 1.8261\n",
      "\n",
      "Epoch 00006: loss improved from 1.87700 to 1.82611, saving model to weights-improvement-06-1.8261.hdf5\n",
      "Epoch 7/16\n",
      "12098/12098 [==============================] - 2689s 222ms/step - loss: 1.7860\n",
      "\n",
      "Epoch 00007: loss improved from 1.82611 to 1.78603, saving model to weights-improvement-07-1.7860.hdf5\n",
      "Epoch 8/16\n",
      "12098/12098 [==============================] - 2729s 226ms/step - loss: 1.7541\n",
      "\n",
      "Epoch 00008: loss improved from 1.78603 to 1.75410, saving model to weights-improvement-08-1.7541.hdf5\n",
      "Epoch 9/16\n",
      "12098/12098 [==============================] - 2741s 227ms/step - loss: 1.7273\n",
      "\n",
      "Epoch 00009: loss improved from 1.75410 to 1.72729, saving model to weights-improvement-09-1.7273.hdf5\n",
      "Epoch 10/16\n",
      "12098/12098 [==============================] - 2749s 227ms/step - loss: 1.7049\n",
      "\n",
      "Epoch 00010: loss improved from 1.72729 to 1.70492, saving model to weights-improvement-10-1.7049.hdf5\n",
      "Epoch 11/16\n",
      "12098/12098 [==============================] - 2765s 229ms/step - loss: 1.6848\n",
      "\n",
      "Epoch 00011: loss improved from 1.70492 to 1.68481, saving model to weights-improvement-11-1.6848.hdf5\n",
      "Epoch 12/16\n",
      "12098/12098 [==============================] - 2757s 228ms/step - loss: 1.6670\n",
      "\n",
      "Epoch 00012: loss improved from 1.68481 to 1.66696, saving model to weights-improvement-12-1.6670.hdf5\n",
      "Epoch 13/16\n",
      "12098/12098 [==============================] - 2773s 229ms/step - loss: 1.6511\n",
      "\n",
      "Epoch 00013: loss improved from 1.66696 to 1.65114, saving model to weights-improvement-13-1.6511.hdf5\n",
      "Epoch 14/16\n",
      "12098/12098 [==============================] - 2768s 229ms/step - loss: 1.6370\n",
      "\n",
      "Epoch 00014: loss improved from 1.65114 to 1.63702, saving model to weights-improvement-14-1.6370.hdf5\n",
      "Epoch 15/16\n",
      "12098/12098 [==============================] - 2785s 230ms/step - loss: 1.6242\n",
      "\n",
      "Epoch 00015: loss improved from 1.63702 to 1.62420, saving model to weights-improvement-15-1.6242.hdf5\n",
      "Epoch 16/16\n",
      "12098/12098 [==============================] - 2777s 230ms/step - loss: 1.6118\n",
      "\n",
      "Epoch 00016: loss improved from 1.62420 to 1.61179, saving model to weights-improvement-16-1.6118.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d37e388b0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(data_X, data_y, epochs=16, batch_size=128, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Generate statement with lstm model with weights what give out the least loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_chars = dict()\n",
    "for ind, char in enumerate(sorted_chars):\n",
    "    ind_chars[ind] = char\n",
    "\n",
    "initializing_sentence = '''There are those who take mental phenomena naively, just as they\n",
    "would physical phenomena. This school of psychologists tends not to\n",
    "emphasize the object.'''\n",
    "initializing_sentence = initializing_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "initializing_sentence = initializing_sentence.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are those who take mental phenomena naively just as they would physical phenomena this school of psychologists tends not to emphasize the object'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after reformat:\n",
    "initializing_sentence = initializing_sentence.lower()\n",
    "initializing_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sentence = []\n",
    "for char in initializing_sentence:\n",
    "    ind_sentence.append(char_ind[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 150 characters in the initializing sentence.\n"
     ]
    }
   ],
   "source": [
    "# report th number of characters in this sentence \n",
    "num_ini_sen = len(ind_sentence)\n",
    "print(f'There are {num_ini_sen} characters in the initializing sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Let's write out the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and the sensedata the sensedata the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sensedata the sensedata and the sen"
     ]
    }
   ],
   "source": [
    "pat = ind_sentence[0:99]\n",
    "for i in range(1000):\n",
    "    x_val = np.reshape(pat, (1, len(pat), 1))\n",
    "    x_val = x_val / float(chars_used)\n",
    "    pred = lstm.predict(x_val, verbose=0)\n",
    "    index = np.argmax(pred)\n",
    "    result = ind_chars[index]\n",
    "    sys.stdout.write(result)\n",
    "    pat.append(index)\n",
    "    pat = pat[1:len(pat)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
